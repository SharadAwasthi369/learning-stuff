{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_layer:\n",
    "    '''\n",
    "    the input_layer should have same dimension to the input\n",
    "    that input=[1,1,1,1,1] 1X5\n",
    "    then input_dim should be 5 to get all the five attributes\n",
    "    and the output dim tells the number of neurons in the neural network\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        self.W=np.random.randn(input_dim,output_dim)\n",
    "        self.b=np.random.randn(1,output_dim)\n",
    "    \n",
    "    \n",
    "    def forward_prop(self,input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.W) + self.b\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    def backward_propagation(self, output_error, lr):\n",
    "        input_error = np.dot(output_error, self.W.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        \n",
    "        \n",
    "        self.weights -= lr * weights_error\n",
    "        self.bias -= lr * output_error\n",
    "        \n",
    "        \n",
    "        return input_error\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the activation layers to use\n",
    "'''\n",
    "\n",
    "class ActivationLayer:\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "error handling functions\n",
    "'''\n",
    "    \n",
    "    def mse(y_true, y_pred):\n",
    "        return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "    def mse_prime(y_true, y_pred):\n",
    "        return 2*(y_pred-y_true)/y_true.size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "activationlayer activations\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return (sigmoid(x))(1-sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    if x>=0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def relu_prime(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            inp = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(inp)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, lr):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                input_data = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(input_data)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, lr)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "            \n",
    "    def score(self,x_test,y_test):\n",
    "        get_data=np.array(predict(x_test))\n",
    "        error=np.abs(y_test-get_data)\n",
    "        error/=y_test\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array([[5,6,1,2],[4,3,2,1]]).reshape(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.array([[1,2,3,4],[7,8,9,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.dot(np.array([1,2,3,7]).reshape(-1,1),np.array([1,2,3,7,8]).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.dot(np.array([[1,2,3,4],[7,8,9,10]]),np.array([[5,6,1,2],[4,3,2,1]]).reshape(4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.70309348  1.56120816]\n",
      " [-1.27053593 -1.16441951]\n",
      " [ 0.42866253  0.58887277]\n",
      " [ 1.15512705 -0.27502095]\n",
      " [-1.08515406 -0.21073441]]\n",
      "[[ 0.83159839 -1.1995768 ]]\n",
      "[[ 0.83159839 -1.1995768 ]]\n"
     ]
    }
   ],
   "source": [
    "d=dense_layer(5,2)\n",
    "print(d.W)\n",
    "print(d.b)\n",
    "print(d.forward_prop([0,0,0,0,0]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
